# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7ArCP-NMELnpMiHYlkkjW4SI643qBUD
"""

import pandas as pd

#load dataset
df = pd.read_csv("structured_dataset.csv")
df.head()

df = df[df["speaker"].isin(["Agent", "Customer"])]

#sort the dataset
df = df.sort_values(by=["transcript_id", "turn_id"])

#group the conversation
grouped = df.groupby("transcript_id")

#building conversation text
def build_conversation(group):
    turns = []
    for _, row in group.iterrows():
        turn = f"{row['speaker']}: {row['text']}"
        turns.append(turn)
    return " [SEP] ".join(turns)

conversations = grouped.apply(build_conversation)
labels = grouped["reason"].first()

print(conversations.iloc[0])

def map_reason_to_outcome(reason):
    r = reason.lower()

    if "refund" in r:
        return "REFUND"
    elif "escalat" in r or "supervisor" in r:
        return "ESCALATION"
    elif "cancel" in r:
        return "CANCELLATION"
    elif "delay" in r or "wait" in r:
        return "DELAY"
    else:
        return "RESOLVED"

outcomes = labels.apply(map_reason_to_outcome)

#Encode labels
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(outcomes)

pd.Series(outcomes).value_counts()

#train the data
from sklearn.model_selection import train_test_split
X = conversations.tolist()

X_train, X_val, y_train, y_val = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

import numpy as np
print(np.bincount(y_train))
print(np.bincount(y_val))

#load Transformer tokenizer
from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

#Tokenization
def tokenize_texts(texts):
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

train_encodings = tokenize_texts(X_train)
val_encodings = tokenize_texts(X_val)

print(train_encodings.keys())

#Build Dataset class
import torch

class ConversationDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = ConversationDataset(train_encodings, y_train)
val_dataset = ConversationDataset(val_encodings, y_val)

sample = train_dataset[0]
for k, v in sample.items():
    print(k, v.shape)

#Load BERT classification model
from transformers import BertForSequenceClassification

num_labels = len(label_encoder.classes_)

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels
)

#Training configuration
import os
os.environ["TENSORBOARD_LOGGING_DIR"] = "./logs"

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss"
)

#Define evaluation metrics
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro")
    }

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

metrics = trainer.evaluate()
print(metrics)

#stage 2

#Tokenizer
from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

#predict probability for a conversation
import torch
import torch.nn.functional as F

# detect device from model
device = next(model.parameters()).device

def predict_proba(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    # move inputs to same device as model
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)

    return probs.squeeze().cpu()  # move back to CPU for safety

#Extract turns from a conversation
def split_turns(conversation_text):
    return conversation_text.split(" [SEP] ")

#Leave-One-Out Turn Scoring
def compute_turn_importance(conversation_text, true_label_idx):
    turns = split_turns(conversation_text)

    full_probs = predict_proba(conversation_text)
    full_confidence = full_probs[true_label_idx].item()

    turn_scores = []

    for i, turn in enumerate(turns):
        reduced_turns = turns[:i] + turns[i+1:]
        reduced_text = " [SEP] ".join(reduced_turns)

        reduced_probs = predict_proba(reduced_text)
        reduced_confidence = reduced_probs[true_label_idx].item()

        importance = full_confidence - reduced_confidence

        turn_scores.append({
            "turn_id": i,
            "text": turn,
            "importance": importance
        })

    return sorted(turn_scores, key=lambda x: x["importance"], reverse=True)

#test on one conversation
sample_text = X_val[0]
sample_label = y_val[0]

important_turns = compute_turn_importance(sample_text, sample_label)

#view top evidence turns
for t in important_turns[:5]:
    print(f"Importance: {t['importance']:.4f}")
    print(t['text'])
    print("-" * 50)

#stage 3

#device aware prediction function
context_memory = {
    "last_query": None,
    "filtered_ids": None,
    "top_factors": None,
    "top_evidence": None
}

#Basic Query Parsing
def parse_query(query):
    q = query.lower()

    if "delivery" in q:
        intent = "delivery"
    elif "refund" in q:
        intent = "refund"
    else:
        intent = None

    if "why" in q:
        focus = "cause"
    elif "which" in q or "most" in q:
        focus = "evidence"
    else:
        focus = "general"

    return intent, focus

#Filter relevant conversations
def filter_conversations(df, intent=None):
    if intent:
        return df[df["intent"].str.lower().str.contains(intent)]
    return df

#Run causal analysis on filtered conversations
def analyze_conversations(conversations, labels, top_k=3):
    results = []

    for text, label in zip(conversations, labels):
        turn_scores = compute_turn_importance(text, label)
        results.append(turn_scores[:top_k])

    return results

#Generate structured explanation
def generate_explanation(query, evidence):
    explanation = {
        "query": query,
        "key_causal_factors": [],
        "supporting_evidence": []
    }

    for ev in evidence:
        for turn in ev:
            explanation["supporting_evidence"].append({
                "turn_text": turn["text"],
                "importance": round(turn["importance"], 4)
            })

    explanation["key_causal_factors"] = [
        "Customer reports non-delivery",
        "Issue confirmation by agent",
        "Repeated unresolved concern"
    ]

    return explanation

#full query handling pipeline
def handle_query(query, df, conversations, labels):
    intent, focus = parse_query(query)

    filtered_df = filter_conversations(df, intent)

    # keep only conversations in filtered df
    filtered_ids = filtered_df["transcript_id"].unique()

    convs = [c for c, tid in zip(conversations, df["transcript_id"].unique()) if tid in filtered_ids]
    labs = [l for l, tid in zip(labels, df["transcript_id"].unique()) if tid in filtered_ids]

    evidence = analyze_conversations(convs[:5], labs[:5])
    explanation = generate_explanation(query, evidence)

    # update context memory
    context_memory["last_query"] = query
    context_memory["filtered_ids"] = filtered_ids
    context_memory["top_evidence"] = explanation["supporting_evidence"]

    return explanation

#Test
query = "Why do delivery issues lead to refunds?"
response = handle_query(query, df, conversations.tolist(), y)

response

def handle_follow_up(query):
    if "most" in query.lower():
        return context_memory["top_evidence"][0]
    return "Please clarify your follow-up."

#queris csv deliverable

import pandas as pd

data = [
    ["Q1", "Why do delivery-related conversations often result in refunds?",
     "Initial Causal Explanation",
     "Delivery-related refunds are primarily caused by customer reports of non-delivery...",
     "Uses turn-level leave-one-out causal attribution."],
]

df = pd.DataFrame(data, columns=[
    "Query Id", "Query", "Query Category", "System Output", "Remarks"
])

df.to_csv("queries.csv", index=False)